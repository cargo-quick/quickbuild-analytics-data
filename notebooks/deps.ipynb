{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargo dependency breakdown\n",
    "\n",
    "## Analogy\n",
    "\n",
    "so to give you an analogy, \n",
    "* MHRA is like a pizza shop where pizzas are made\n",
    "* there are many chefs working in the MHRA pizza shop and there are many pizza shops doing similar work to MHRA\n",
    "* a pizza (backend web server) has cheese (http library) and pizza base (database library)\n",
    "  * pizza base has dough (low-level database library)\n",
    "    * dough has flour (network library), salt (encryption library) and yeast (database query formatter) \n",
    "  * cheese has milk (http library) and salt (same encryption library)\n",
    "* *but*\n",
    "  * there are lots of different ways to make pizza base\n",
    "    * there are lots of different ways to make flour\n",
    "    * there are lots of different ways to make yeast\n",
    "  * there are lots of different ways to make cheese\n",
    "    * there are lots of different ways to make milk\n",
    "    * there are lots of different ways to make salt\n",
    "* it costs money to *pre-make different types of* dough, cheese and pizza base so we want to only *pre-make* the ingredients for the pizzas that people want\n",
    "* everything pre-built has a shelf life of 6 weeks (trust me on this one)\n",
    "* *so*\n",
    "* We want to pre-make only the most popular ingredients to maximise the *amount of time saved by* chefs making pizzas\n",
    "* and then we can have quick pizza in rust land for the masses and no one needs to go hungry again\n",
    "* ...does that make sense??\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "print(pio.renderers.default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = pandas.read_parquet(\"../subtrees-clean.parquet\")\n",
    "\n",
    "tree_counts = trees.groupby(\n",
    "    ['package_name', 'package_version', 'hash'],\n",
    ").agg(\n",
    "    tree_size=('deps_count', 'first'),\n",
    "    example_repo_path=('repo_path', 'first'),\n",
    "    tree_occurrences=('repo_path', 'count'),\n",
    ")\n",
    "\n",
    "version_counts = tree_counts.groupby(\n",
    "    ['package_name', 'package_version'],\n",
    ").agg(\n",
    "    version_occurrences=('tree_occurrences', 'sum'),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_cached(fn):\n",
    "    import inspect\n",
    "    import hashlib\n",
    "    digest = hashlib.sha256(inspect.getsource(get_downloads_data).encode()).hexdigest()\n",
    "    cache_filename = f\"../cache/{fn.__name__}-{digest}.parquet\"\n",
    "\n",
    "    try:\n",
    "        data = pandas.read_parquet(cache_filename)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    data = fn()\n",
    "    data.to_parquet(cache_filename)\n",
    "    # TODO:\n",
    "    # * Delete every file matching f\"../cache/{fn.__name__}-*.parquet\"\n",
    "    #   other than `cache_filename`.\n",
    "    #   (or touch the file we used, if we want an LRU cache with size bigger than 1)\n",
    "    # * Log cache misses and timings.\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_downloads_data():\n",
    "    conn = psycopg2.connect(\n",
    "        database=\"cratesio\",\n",
    "    )\n",
    "    downloads = pandas.read_sql_query(\"\"\"\n",
    "        select\n",
    "            c.name as package_name, v.num as package_version, d.downloads\n",
    "        from\n",
    "            version_downloads as d\n",
    "        join\n",
    "            versions as v on v.id = d.version_id\n",
    "        join\n",
    "            crates as c on c.id = v.crate_id\n",
    "        where\n",
    "            date = '2021-03-29'\n",
    "        order by\n",
    "            package_name, package_version\n",
    "        ;\n",
    "    \"\"\", conn)\n",
    "    return downloads\n",
    "\n",
    "downloads = call_cached(get_downloads_data).set_index(['package_name', 'package_version'])\n",
    "\n",
    "downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = combined = tree_counts.join(\n",
    "    version_counts,\n",
    "    how='inner',\n",
    "    on=['package_name', 'package_version']\n",
    ").join(\n",
    "    downloads,\n",
    "    how='inner',\n",
    "    on=['package_name', 'package_version'],\n",
    ")\n",
    "combined['estimated_daily_downloads'] = (\n",
    "    combined['downloads'] * combined['tree_occurrences'] / combined['version_occurrences']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_unscaled(df, *, x, y):\n",
    "    plot_data = df.reset_index().drop_duplicates(subset=[x, y])\n",
    "    fig = plotly.express.scatter(\n",
    "        plot_data, x='tree_occurrences', y='tree_size',\n",
    "        # hover_name='package_name',\n",
    "    )\n",
    "    fig.update_traces(hovertemplate=None, hoverinfo='skip')\n",
    "    return fig\n",
    "\n",
    "plot_unscaled(combined, x='tree_occurrences', y='tree_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loglog(df, *, x, y, hover_name='package_name'):\n",
    "    df = df.reset_index().drop_duplicates(subset=[x, y])\n",
    "    plot_data = df.reset_index()\n",
    "    fig = plotly.express.scatter(\n",
    "        plot_data, x=x, y=y, hover_name=hover_name,\n",
    "        log_x=True, log_y=True,\n",
    "    )\n",
    "    # fig.update_traces(hovertemplate=None, hoverinfo='skip')\n",
    "    return fig\n",
    "\n",
    "plot_loglog(combined, x='tree_occurrences', y='tree_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loglog(combined, x='estimated_daily_downloads', y='tree_size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "the utility gained by building a package tree is proportional to the number of users (tree_ocurrences) * the size of the tree (tree_size).\n",
    "\n",
    "    utility = tree_ocurrences * tree_size\n",
    "    log(utility) = log(tree_ocurrences * tree_size)\n",
    "    log(utility) = log(tree_ocurrences) + log(tree_size)\n",
    "\n",
    "therefore, lines of constant utility are straight downwards-sloping lines in the above log plot (we want to build the packages that are furthest to the top-right on this plot, like `frame-benchmarking`, `mio` and `winapi`)\n",
    "\n",
    "In practice, tree_ocurrences only says how many *GitHub Repos* are using particular configurations of the crates. We want to know how many *people* are using them. For this, we need to use the crates.io download data.\n",
    "\n",
    "## Caveats\n",
    "\n",
    "The set of requested features do not seem to appear in the lockfile. They only appear in Cargo.toml. In a lot of cases, changing features will add extra dependencies, but not always. We may need to re-do the analysis and create a new `subtrees-clean.parquet` that adds a \"features\" field to each dependency in the tree. This could be a lot of work with not great payoff though. Might be best to just keep in mind that the fragmentation will be a bit worse than what you see on these graphs.\n",
    "\n",
    "I am assuming that build time is proportional to dependency tree size. In reality, it is likely to also scale proportional to crate size (available as `versions.crate_size` in the crates.io postgresql dump), and whether it is using a lot of proc-macros and generics from its dependencies. Predicting crate build times would be a really interesting project, if anyone has a dataset (maybe crater has one?)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('qb-ATkvlelS-py3.9': poetry)",
   "name": "python394jvsc74a57bd0c9a4faaead04bde3041271e375671efe48c7c201b3423c55c116e9b87adb6cf8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "metadata": {
   "interpreter": {
    "hash": "3be3f79b0d6c224c67b74ed101902d04f22489be2ee66d3ec7a43c28aa2c18b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
