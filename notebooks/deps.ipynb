{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargo dependency breakdown\n",
    "\n",
    "This notebook explores the relationship between dependencies and download counts in a bunch of Cargo.lock files found in the wild, in a crates.io database dump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "print(pio.renderers.default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a breakdown of dependency sub-trees found in Cargo.lock files in the wild.\n",
    "trees = pandas.read_parquet(\"../subtrees-clean.parquet\")\n",
    "\n",
    "tree_counts = trees.groupby(\n",
    "    ['package_name', 'package_version', 'hash'],\n",
    ").agg(\n",
    "    tree_size=('deps_count', 'first'),\n",
    "    example_repo_path=('repo_path', 'first'),\n",
    "    tree_occurrences=('repo_path', 'count'),\n",
    ")\n",
    "\n",
    "version_counts = tree_counts.groupby(\n",
    "    ['package_name', 'package_version'],\n",
    ").agg(\n",
    "    version_occurrences=('tree_occurrences', 'sum'),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_cached(fn):\n",
    "    \"\"\"\n",
    "    This is a little cacheing pseudo-decorator, that I use to make things a bit\n",
    "    quicker when re-running the notebook from scratch.\n",
    "    It also means that you don't need to be running a postgresql server with the\n",
    "    crates.io dump as long as you don't change any of the sql queries.\n",
    "    \"\"\"\n",
    "    import inspect\n",
    "    import hashlib\n",
    "    digest = hashlib.sha256(inspect.getsource(get_downloads_data).encode()).hexdigest()\n",
    "    cache_filename = f\"../cache/{fn.__name__}-{digest}.parquet\"\n",
    "\n",
    "    try:\n",
    "        data = pandas.read_parquet(cache_filename)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    data = fn()\n",
    "    data.to_parquet(cache_filename)\n",
    "    # TODO:\n",
    "    # * Delete every file matching f\"../cache/{fn.__name__}-*.parquet\"\n",
    "    #   other than `cache_filename`.\n",
    "    #   (or touch the file we used, if we want an LRU cache with size bigger than 1)\n",
    "    # * Log cache misses and timings.\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_downloads_data():\n",
    "    \"\"\"\n",
    "    We care about how often dependency subtrees appear in our public Cargo.lock dataset\n",
    "    (to see how fragmented the configurations for each package are),\n",
    "    but we also want to scale by how many times the various packages get downloaded,\n",
    "    to give us the overall weight for each crate. This function gives us these numbers.\n",
    "    \"\"\"\n",
    "    conn = psycopg2.connect(\n",
    "        database=\"cratesio\",\n",
    "    )\n",
    "    downloads = pandas.read_sql_query(\"\"\"\n",
    "        select\n",
    "            c.name as package_name, v.num as package_version, d.downloads\n",
    "        from\n",
    "            version_downloads as d\n",
    "        join\n",
    "            versions as v on v.id = d.version_id\n",
    "        join\n",
    "            crates as c on c.id = v.crate_id\n",
    "        where\n",
    "            date = '2021-03-29'\n",
    "        order by\n",
    "            package_name, package_version\n",
    "        ;\n",
    "    \"\"\", conn)\n",
    "    return downloads\n",
    "\n",
    "downloads = call_cached(get_downloads_data).set_index(['package_name', 'package_version'])\n",
    "\n",
    "downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = combined = tree_counts.join(\n",
    "    version_counts,\n",
    "    how='inner',\n",
    "    on=['package_name', 'package_version']\n",
    ").join(\n",
    "    downloads,\n",
    "    how='inner',\n",
    "    on=['package_name', 'package_version'],\n",
    ")\n",
    "combined['estimated_daily_downloads'] = (\n",
    "    combined['downloads'] * combined['tree_occurrences'] / combined['version_occurrences']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_unscaled(df, *, x, y, hover_name='package_name', color=None):\n",
    "    plot_data = df.reset_index().drop_duplicates(subset=[x, y])\n",
    "    fig = plotly.express.scatter(\n",
    "        plot_data, x=x, y=y, color=color,\n",
    "        hover_name=hover_name,\n",
    "    )\n",
    "    if hover_name is None:\n",
    "        fig.update_traces(hovertemplate=None, hoverinfo='skip')\n",
    "    return fig\n",
    "\n",
    "plot_unscaled(combined, x='tree_occurrences', y='tree_size', hover_name=None)\n",
    "\n",
    "# tree_size is the number of crates in the dependency subtree\n",
    "# tree_occurrences is a count of how many Cargo.lock files a particular dependency sub-tree exists in\n",
    "# This plot isn't super-useful. Skip to the next one for better axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loglog(df, *, x, y, hover_name='package_name', color=None):\n",
    "    plot_data = df.reset_index().drop_duplicates(subset=[x, y])\n",
    "    fig = plotly.express.scatter(\n",
    "        plot_data, x=x, y=y, hover_name=hover_name, color=color,\n",
    "        log_x=True, log_y=True, \n",
    "    )\n",
    "    # fig.update_traces(hovertemplate=None, hoverinfo='skip')\n",
    "    return fig\n",
    "\n",
    "plot_loglog(combined, x='tree_occurrences', y='tree_size')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `tree_size` is the number of crates in the dependency subtree\n",
    "* `tree_occurrences` is a count of how many Cargo.lock files a particular dependency sub-tree exists in\n",
    "* Things out in the bottom-right are leaf-dependencies that are depended upon by loads of people\n",
    "* Things in the top-left are huge dependency trees that only show up in one Cargo.lock file\n",
    "  (note that there are a bunch of copies of amethyst and actix-web, with subtly different versions\n",
    "  of their transitive sub-dependencies).\n",
    "\n",
    "Note that the x axis is not scaled by how many times I expect particular trees to be built - just by how many public repos contain the tree.\n",
    "\n",
    "\n",
    "PNG version of the figure is copied below for people viewing on GitHub. The real thing has the names of tree-root crates on hover.\n",
    "![](./tree_size-vs-tree_occurrences.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loglog(combined, x='estimated_daily_downloads', y='tree_size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same graph, but scaled by how many times each crate is downloaded on crates.io. For each crate-version, we count how many times it appears at a dependency sub-tree root, and divide its crates.io download count by this number.\n",
    "\n",
    "PNG version of the figure is copied below for people viewing on GitHub. The real thing has the names of tree-root crates on hover.\n",
    "\n",
    "![](tree_size-vs-esitmated_daily_downloads.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "the utility gained by building a package tree is proportional to the number of users (tree_ocurrences) * the size of the tree (tree_size).\n",
    "\n",
    "    utility = tree_ocurrences * tree_size\n",
    "    log(utility) = log(tree_ocurrences * tree_size)\n",
    "    log(utility) = log(tree_ocurrences) + log(tree_size)\n",
    "\n",
    "therefore, lines of constant utility are straight downwards-sloping lines in the above log plot (we want to build the packages that are furthest to the top-right on this plot, like `frame-benchmarking`, `mio` and `winapi`)\n",
    "\n",
    "In practice, tree_ocurrences only says how many *GitHub Repos* are using particular configurations of the crates. We want to know how many *people* are using them. For this, we need to use the crates.io download data.\n",
    "\n",
    "## Caveats\n",
    "\n",
    "The set of requested features do not seem to appear in the lockfile. They only appear in Cargo.toml. In a lot of cases, changing features will add extra dependencies, but not always. We may need to re-do the analysis and create a new `subtrees-clean.parquet` that adds a \"features\" field to each dependency in the tree. This could be a lot of work with not great payoff though. Might be best to just keep in mind that the fragmentation will be a bit worse than what you see on these graphs.\n",
    "\n",
    "I am assuming that build time is proportional to dependency tree size. In reality, it is likely to also scale proportional to crate size (available as `versions.crate_size` in the crates.io postgresql dump), and whether it is using a lot of proc-macros and generics from its dependencies. Predicting crate build times would be a really interesting project, if anyone has a dataset (maybe crater has one?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw a line at 20% build-time-saved\n",
    "\n",
    "How many packages do we need to build in order to save 20% of people's time?\n",
    "\n",
    "Assuming:\n",
    "* Each crate is compiled once immediately after download and then never again (not true, but the repeated compiles probably scale with the number of downloads, so I'm hoping it falls out in the wash)\n",
    "* !!! Each crate takes a time proportional to its dependency count to compile (!!! I don't think that this is valid, because it assumes that there will be build time incurred by leaf dependencies at each layer above, even though the leaf dep's build time has already been counted based on the download count of the leaf dependency)\n",
    "* !!! It is possible to build a crate without building its leaf-dependencies (!!! In practice, in order to build a level-above crate, you need to reach into the quagmire and build the leaf-dependencies, even if we haven't identified them as good things to build)\n",
    "\n",
    "Let's just get a graph drawn and go from there, shall we?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plot_loglog(combined, x='estimated_daily_downloads', y='tree_size')\n",
    "\n",
    "combined['time_wasted_compiling'] = combined['estimated_daily_downloads'] * combined['tree_size']\n",
    "time_wasted_compiling = combined['time_wasted_compiling']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loglog(combined, x='estimated_daily_downloads', y='tree_size', color='time_wasted_compiling')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PNG included below for people viewing on GitHub.\n",
    "\n",
    "![](./time_wasted_compiling.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_wasted_compiling.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_wasted_compiling = time_wasted_compiling.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_cost = time_wasted_compiling.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_cost[-1] / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['in_25_percent'] = cumulative_cost < (cumulative_cost[-1] / 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loglog(combined, x='estimated_daily_downloads', y='tree_size', color='in_25_percent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['in_25_percent'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we give ourselves a goal of saving 25% of all computation done by `cargo build` invocations?\n",
    "\n",
    "PNG for GitHub users:\n",
    "![](./in_25_percent.png)\n",
    "\n",
    "we would need to build 1728 package trees, and can skip 574515 trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# But what if we only care about downloads?\n",
    "\n",
    "Editor's note: I can't remember why I thought that this was a valid simplifying assumption. It may not actually be valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_daily_downloads = combined['estimated_daily_downloads'].sort_values(ascending=False).cumsum()\n",
    "combined['cumulative_daily_downloads'] = cumulative_daily_downloads\n",
    "in_25_percent_downloads = cumulative_daily_downloads < (cumulative_daily_downloads[-1] / 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_25_percent_downloads.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['in_25_percent_downloads'] = in_25_percent_downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_unscaled(combined, x='estimated_daily_downloads', y='cumulative_daily_downloads', color='in_25_percent_downloads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we would only need to build 116 packages for 25% coverage. I'm not sure how valid this is though.\n",
    "\n",
    "![](./downloads_25.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined[in_25_percent_downloads]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined[in_25_percent_downloads].to_csv('../top_25_percent_downloads.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things that could screw me over:\n",
    "\n",
    "* If a package version shows up in the crates.io download counts, but not in the github scraped lockfiles, I ignore it completely.\n",
    "* I still don't take into account features that don't affect the shape of the dependency tree. \n",
    "* I've not had very good sleep for 2 days straight, so I've probably made a mistake somewhere"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('qb-kT1npgut-py3.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "metadata": {
   "interpreter": {
    "hash": "3be3f79b0d6c224c67b74ed101902d04f22489be2ee66d3ec7a43c28aa2c18b8"
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "3a8d76b211de6059b3d0ba0b2306e8a47828e5dac7212b725c898b86c848d937"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
